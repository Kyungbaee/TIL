- NLP
- strmming의 경우 활용형들을 원형 하나로 변환해 용량이 줄어듬
- RNN (순환 신경망)
  - one to one
  - one to many
  - many to one
  - many to many
- 시퀀스 데이터 (sequence data)
- 셀(cell): RNN의 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드 (RNN의 반복 단위, 개별)
- 메모리셀(memory cell): 이전의 값을 기억하는 셀 (RNNcell, 전체)
- 은닉상태(hidden state): 은닉층의 메모리 셀에서 나온 값이 출력층 방향 또는 다음 시점의 자신에게 보내는 상태
- BPTT(Back-Propagation Through Time)
  - 타임 스텝별로 네트워크를 펼친 후 사용
  - 현재 시간의 오차를 과거 시간의 상태까지 역전파
- 워드 임베딩(Word Embeding): 단어를 특정 차원의 벡터로 바꾸어 주는 것 -> 벡터화
- 시퀀스 방식의 인코딩 -> 패딩
- NLP의 첫 층은 임베딩(Embedding) 층

시퀀스 데이터: 순서가 존재하는 데이터셋(ex: 문장, 대화, 주가데이터, 동영상, 생체데이터)
RNN(Recurrent Neural Network) : RNN은 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델

RNN의 유형
- One to one : 가장 기본적인 모델(simple RNN)
- One to many : 하나의 이미지를 여러 문장으로 표현 가능
- Many to one : 여러 영화 리뷰를 하나의 평가(긍정, 부정) 분류 가능
- Many to many : 여러 단어를 입력 받아 여러 개의 단어로 구성된 문장을 구사(주가 데이터도 가능)

RNN의 한계점: 기울기 소실... and 장기 의존성...

워드 임베딩(Word Embedding) : 단어를 특정 차원의 벡터로 바꿔주는 것

pad_to_sequences : 문장의 시퀀스 길이를 맞춰주기 위해서 앞으로나 뒤로 0 패딩을 넣어준다.

기존 모델의 한계: 값이 오직 출력층 방향으로 향함
결과값을 출력층 방향으로 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 
셀 : 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할(RNN의 반복 단위, 개별)
메모리셀 : 이전의 값을 기억하는 일종의 메모리 역할을 수행하는 셀(전체, RNNcell)
은닉상태 : 은닉층의 메모리 셀에서 나온 값이 출력층 방향 또는 다음 시점의 자신에게 보내는 상태

역전파
BackPropagation Through Time
역전파를 통해 가중치 비율 조정하여 오차 감소 진행

이전 작업을 현재 작업과 연결할 수 있다
은닉층 데이터를 저장하여 펼쳐진 형태의 순환구조로 확률값 계산

Padding
문장 길이를 같게 만들어 하나의 행렬로 보고 한꺼번에 처리할 수 있게 해줌

어간 추출(Stemming)
장점
- 용량을 줄일 수 있다(주된 목적)
- 비슷한 의미라는 것을 모델에 인식시킬 수 있다


RNN(순환신경망)
셀(cell): 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할 (기존의 노드 역할과 비슷)
메모리셀(memory cell) : 이전의 셀을 기억하는 일종의 메모리 역할 
은닉상태(hidden state): 은닉층의 메모리셀에서 나온 값이 출력층 방향 또는 다음 시점의 자신에게 보내는 상태
추상화가 잘 되어 있으면 내부를 알기는 어렵지만 사용하기가 좋음

RNN에서는 주로 tanh를 사용함
tanh의 단점? 양끝으로 가게 되면 값이 너무 작아지거나 값이 너무 커지는 기울기 소실 혹은 기울기 폭주 문제

시퀀스 방식으로 텍스트를 인코딩할 때 패딩을 사용하는 이유?
문장의 길이가 서로 다를 수 있는데, 모델은 길이가 전부 동일한 문서들에 대해 하나의 행렬로 보고, 한꺼번에 묶어 처리하기 때문에
padding으로 빈부분을 채워서 길이를 맞춰줍니다.


Tokenizer
oov_token: out-of-vocabulary token , 없는 단어가 들어왔을 때 oov_token으로 들어오게 된다
단어장에 없는 단어들은 <oov>토큰으로 치환!
