log = 정의역이 양수일 때 정의된 함수, 표본에 음수 있으면 최소값보다 큰 양수 더해서 보정
equal width binning 
= histogram과 유사, 구간에 따라 표본 개수가 다르게 산정 가능/절대평가
equal frequency binning = 상대평가/ 표본 개수가 같게 산정되도록 구간 산정

plt.subplots(행, 열) -> 그래프 한꺼번에 그리기
fig, axes = plt.subplots(행, 열)에서 fig, axes 이렇게 변수를 지정해 사용하는 이유?
애초에 변수가 tuple 타입으로 존재 -> 변수[0] = fig, 변수[1] = axes로 각각 받아주기 위함

index_col =“Id”로 받으면 좋은 효과
- set_index와 유사
- Id는 고유값 = unique값 -> 표본 및 그룹별 구분 가능

pd.read_csv("", index_col="Id")

feature engineering
과적합 방지

fit_transform : train
transform : train, test

standard scaler    minmax scaler     robust sclaer
std = 1           min = 0                  표준화 후 동일한 값을 더 넓게 분포 시키고 있음
mean = 0          max = 1            median이 0 →　+, - 절반
                                                 아웃라이어 영향 최소화
scale 전후 분포는 동일, x값만 달라짐

log transform
표준정규분포 형태로 → 일반적인 예측 성능↑

  이산화(discretisation)
구간으로 grouping
  Equal width binning       동일 길이로 나누기 cut
  Equal frequency binning   동일 개수로 나누기 qcut

fig, axes = plt.subplots(1, 2)
(figure, AxesSubplot)

  Encoding
Categorigal Feature → Numerical Feature
- Ordinal : 비교 쉬움, 메모리 적음 vs 순서에 의미가 생길 수 있음
   train["col"].astype("category").cat.codes
   OrdinalEncoder().fit(train[["col"]]).transform(train[["col"]])
- One-Hot : 메모리 많이 차지, 오래 걸림
  pd.getdummies(train["col"])
  train[ohe.categories[0]] = OneHotEncoder().fit(train[["col"]]).transform(train[["col"]]).toarray()

  파생 변수
  다항식 전개에 기반한 파생변수
    feature[a, b]에서 최대 차수가 2일 때
    1, a, b, ab, a² b² → uniform, 범위가 너무 작아 특징 찾기 어려운 값에서 사용
  PolynomialFeatures(degree=2).fit_transform(train[["col1", "col2"]])
  pd.DataFrame(pf_val, columns=pf.get_feature_names_out())

  변수 선택
  ✔︎ 상관이 높은 두 변수 → 하나만
  ✔︎ 분산 기반 선택 ... 값이 대부분 하나의 값이면 채택✕

  왜도, 첨도
왜도: 비대칭도
중앙값 = 평균 : 0
중앙값 < 평균 : +
중앙값 > 평균 : -

첨도: 뾰족한 정도(중심에 분포)
3에 가까우면 정규
3보다 작으면 납짝이

- 범위 변환(Scaling)
  - fit은 train에만 진행하고, test에는 진행하지 않음
  - transform은 양쪽 모두에 적용
  - fit_transform을하고 train에만 transform을하는 방식
  - 2개의 데이터가 합쳐져있는 경우에는 함께 진행
  - 각각의 평균과 표준편차가 다르기 때문에, 0과 1로 변환된 평균과 표준편차 값이 서로 다른 값을 의미하게 됨
- 범주화(Binning): 연속형 변수 -> 범주형 변수
  - Equal width binning (_cut)
    - 가능한 값의 범위를 동일한 너비의 N개의 빈으로 나눔, 편향된 분포에 민감
  - Equal frequency binning (_qcut)
    - 변수의 가능한 값 범위를 N개의 빈으로 나눔 (각 빈은 동일한 양의 관측값), 알고리즘의 성능을 올리지만 관계를 방해할 가능성이 있음
- 숫자화(Dummy): 범주형 변수 -> 연속형 변수
  - 인코딩(Encoding) -> Ordianl-Encoding, One-hot Encoding
    - Categorical Feature의 고유값들을 임의의 숫자로 바꿈 (Ordinal-Encoding)
- 정규분포 형태가 모델 학습에 도움이 됨
- 파생변수 생성 (Feature Generation)
  - 이미 존재하는 변수로부터 여러가지 방법을 이용해 새로운 변수를 만들어낼 수 있음
  - 적합한 파생변수는 모델 성능 향상에 도움이 됨

1.각 기준으로 transform 해주기 위해서는 train 에만 fit을 해주거나
2.train, test 를 합친 후에 fit을 해줍니다.
1번이 실용성이 크다.
라이브 상황에서 모든 데이터를 다 학습시킬 수 없다. ex)계속 데이터가 업데이트
fit 을 할 때는 train, test 에 둘 다 해야 한다? train 데이터 셋에만 해준다. 합쳐서 한번에 해주면 모르지만, 각각 해줄 경우 기준이 달라진다.
test는 transform만 해준다

SS = mean 0, std 1
mm = 모든 데이터가 0~1로 변환
robust scaler는 median이 0이므로 데이터의 절반은 양수, 절반은 음수로 생각할 수 있습니다.¶

연속된 수치데이터를 범주형으로 바꿔주는것이 비닝이다 ex) 희솟값들을 모아서 '기타' 처럼 퉁쳐서 사용한다

인코딩 - 문자 범주형 데이터를 수치형으로 바꿔보자
ordinal 인코딩의 장점? 순서성이 있어서 비교하기가 쉽습니다
ordinal 인코딩의 단점? 정보가 지나치게 축약될 수 있고, 단계별 간격의 의미가 애매하다?

어떤 변수를 제곱을 하면 좋을까요? 너무 범위가 작은 변수
uniform 한 분포는 무엇일까요?변화폭이 크지않고 균등한 분포
피쳐엔지니어링은 정형데이터를 다룬다

o Scaler
 - (종류) StandardScaler(), MinMaxScaler(), RobustScaler()
 - (방법) 같은 기준으로 transform, 1) train에만 fit, 2) train+test에 fit
   * ss.fit(train[["SalePrice"]]).transform(train[["SalePrice"]])
   * ss.fit_transform(train[["SalePrice"]])
 - (목적) 피처들의 범위를 변형해 피처들 간의 영향력을 균등화, 처리속도 빠르게!

o Binning
 - Numerical Feature만으로는 경향을 보기 어려울 때, 20~29세까지를 20세로 묶는 등

o 이산화(Discretization)
 - Numerical을 categorical로 변환
 - cut(동일한 길이로 나누기) vs qcut(동일한 개수로 나누기)

o 인코딩(Scikit Learn 중 categorical → numerical)
 - (사용) encoding.fit().transform() or encoding.fit_transform()
  → from sklearn.preprocessing import OrdinalEncoder
  → from sklearn.preprocessing import OneHotEncoder
 - (종류) Ordinal encoding(oe, 1-2-3-4-..., 순서의미 있을경우), OneHotEncoding(ohe, 칼럼별 해당여부..) 등

o 파생변수 중 다항식 전개(Polynomial Expansion)
 - Degree(차수) 값에 기반한 다항식 전개를 통해 파생변수 생성
 - bias는 feature들의 0승

Feature Engineering

- binning
    - 연속된 수치형을 도수분포로 만드는 histogram에서 사용한 것과 비슷한 의미이다.
        - cut, qcut
- uniform한 분포: 변화폭이 크지않고 균등한 분포
    - 너무 uniform한 분포는 오히려 특징을 파악하기 어려울 때 polynominal features(다항식 차수)를 사용하면 음수를 없애거나, 너무 범위가 작은 변수를 크게 키워 특징을 파악할 수 있다.

이산화(cut과 qcut)

- cut → 절대평가(동일한 길이로 나누기)
- qcut → 상대평가(동일한 개수로 나누기)

인코딩
- lightGBM에서는 행렬을 묶어서 one-hot-encoding의 단점을 보완해준다.
- 차원 축소하는 알고리즘을 써서 2, 3차원으로 압축할 수 있다. (사진 압축)
- predict_proba 로 예측한 결과값을 평가할 수 있다. (추천시스템)

1. Scaling : 방식에 따라 최종 분포의 특징이 달라짐
        - StandardScaler : mean = 0, std = 1
        - MinMaxScaler : min = 0, max = 1
        - RobustScaler : median = 0
2. Encoding
        - OrdinalEncoding
                1) 장점 : 순서가 있어 이해/비교가 쉬움
                            하나의 컬럼으로 표현 가능 - 메모리 관리에 유리
                2) 단점 : 정보가 지나치게 축약될 수 있음
                            순서에 따른 의미 부여가 중요
        - OneHotEncoding
                1) 장점 : feature 간에 불필요한 관계성이 없음
                2) 단점 : 지나치게 많은 컬럼, 희소값이 생길 수 있음 
                        -> 차원 축소와 같이 볼 수 있음
        - LabelEncoding
3. 다항식 전개 Polynomial Expansion
- 설정한 degree에 맞게, 파생 변수를 생성 (if degree = 2 : 1, a, b, a^2, ab, b^2 -> 6개 변수 생성)
- 단점     1) 변수를 직관적으로 이해하기 어려움
- 장점     1) 음수 제거 가능
        2) 범위가 너무 작거나, 변화폭이 너무 작아 그 차이를 알기 어려운 변수 사이의 차이를 모델이 인식
           -> 여러 개 feature를 사용하면서, 과대적합을 줄이고 머신러닝 안정성 개선 가능변수 선택
4. 상관관계 기반 feature selection : 지나치게 상관관계가 높은 변수들은 하나만 선택해도 됨
