# DT 중 ensenble 중 부스팅 모델

o Decision Tree 모델 발전과정
o XGBoost 모델
o LightGBM 모델
- 행과 열을 light하게 만듬(GOSS, EFB)
 * (GOSS) 행을 light, (EFB) 열을 light하게 Bundling
- 작은 데이터에 Overfitting되기 쉬움
o CatBoost
- category column을 전처리하는 기능을 갖춤
- params → cat_features

#분류 평가

o Confusion Matrix(혼동 행렬)
 - Scikit Learn 기준으로 보겠다
 - (지표) 정밀도&재현율 상호보완적 
  * 1) Precision(정밀도) = TP / (TP+FP)
  * 2) Recall(재현율) = TP / (TP+FN)
 - (구분) TP, FP, TN, FN
  * (TP) 맞다고 예측(P)해서 맞은것(T) 
  * (TN) 안맞는다고 예측(N)해서 맞은것(T)
  * (FP) 맞다고 예측(P)해서 틀린것(F) / 1종오류
  * (FN) 안맞는다고 예측(N)해서 틀린것(F) / 2종오류
 - AUC-ROC Curve: 다양한 임계값에서 모델의 분류 성능 측정 그래프
  * y축-TPR(민감도), x축-FPR(1-specificity)
  * AUC는 곡선아래 면적(1이 될수록 이상적)

o F1 score
 - precision과 recall의 조화평균
 - 주로 분류 클래스 간의 데이터가 불균형이 심각할때 사용
 - 높을수록 좋은 모델

선형 회귀
간단하고 빠름
수치형, 경향성 뚜렷한 데이터

결정 트리
화이트박스(시각화), Overfitting

Bagging
RandomForest
n_estimator 많을수록 좋다..
ExtraTree
극도로 무작위

Boosting
Gradient Boosting Tree
loss : squared       absolute      hubber
       loss 변화✕                 이상치 덜 민감

Light GBM
  Gradient based One side sampling  행을 줄여준다..🤔
  큰 graident 가진 인스턴스 유지(오차가 큰 data)
  작은 gradient 인스턴스 무작위로 sampling

  Exclusife Feature Bundling
  feature bundle

  Leaf-wise tree
  max delta loss를 가지는 leaf node를 분할

CatBoost
  ordered boosting
  순열 기반 대안으로 범주형 기능 해결
 
XGB
  시각화 기능 xgb.plot_tree(model)

  불균형 데이터 

  평가지표 
    희소한 데이터를 정확하게 예측하는 게 중요한 경우
    Confusion Matrix
           예측 0   1
       실제
        0     TN   FP(1종 오류)
        1     FN   TP
           (2종 오류)
    precision   1로 예측한 것 중 맞게 예측한 것   tp/tp+fp
    recall      실제값이 true 중 맞게 예측한 것  tp/tp+fn
    accuracy    True/전체

    classification_report
    ROC curve, AUC curve

    resampling
     - under-sampling 많은 쪽에서 일부 sampling
     - over-sampling  적은 쪽을 늘려 비율 맞춤
          Synthetic Minority Over sampling Technique
          k-근접이웃
          
          
- 랜덤포레스트 (배깅) -> 무작위성
- 그래디언트 부스팅 트리 -> 무작위성 없이, 이전 트리를 개선하는 방식
- GOSS(Gradient bassed One Side Sampling) -> 행을 어떻게 줄일 것인지
- EFB(Exclusive Feature Bundling) -> 열을 어떻게 줄일 것인지
- CatBoost: 대칭트리, 예측 시간 감소
- XGBoost: BFS처럼 넓게 형성
- LightGBM: DFS처럼 깊게 형성
- Confusion Matrix (혼돈 행렬)
  - precision(정밀도): 1로 예측한것중 진짜 1인것
  - recall(재현율): 실제값이 1인것 중에 맞게 추론한것
  - specificity(특이도)
- Resampling
  - Oversampling (imbalanced-learn)
    - SMOTE(Synthetic Minority Over-sampling Technique)
    - 합성 소수자 오버 샘플링 기법
    - K-근접 이웃 이용
  - Undersampling
    - 구현은 쉽지만, 성능 저하 가능성이 있음
    - 부트스트래핑
- ravel / flatten

fp(false positive) - 음성인데 양성이라고 하는 것.
tn(True Negative) - 음성인데 음성이라고 맞춘 것.
fn(False Negative) - 양성인데 음성이라고 하는 것.
tp(True Positive) - 양성인데 양성이라고 맞춘 것.

p = 모델이 예측 값이 true
n = 모델이 예측 값이 false

fp - 1종오류 - 임신이 아닌데 임신이라고 하는것. (억울한 케이스)
fn - 2종오류 - 임신이 맞는데 임신이 아니라고 하는것.

Gradient Boosting Tree
1. 랜덤 포레스트와 다르게 무작위성이 없다.
2. 매개변수를 잘 조정해야 하고 훈련 시간이 길다.
3. 데이터의 스케일에 구애받지 않는다.
4. 고차원의 희소한 데이터에 잘 작동하지 않는다.

lightGBM
 - 머신러닝을 위한 오픈 소스 분산 그래디언트 부스팅 프레임워크
CatBoost
 - 오픈소스 소프트웨어 라이브러리
 - 순열 기반 대안을 사용하여 범주형 기능을 해결하려고 시도하는 그래디언트 부스팅 프레임워크
XGBoost
* 장점
 - GBM 대비 빠른 수행시간(병렬처리)
 - 과적합 규제
 - Early Stopping 기능이 있음
* 단점
 - 학습시간이 느림
 - Hyper Parameter 수가 많아 시간이 오래걸림
LightGBM
 - GOSS와 EFB를 적용한 LightGBM은 XGBoost와 비교해 학습시간만을 단축시킨 모델
단점
 - 과적합에 민감하고 작은 데이터에 대해 과적합되기 쉬움
CatBoost
* 장점
 - 과적합을 극복하기 위해 부스팅 주문
 - 더 빠른 실행을 위해 Oblivious Trees 또는 Symmetric Trees 사용
* 단점
 - 희소행렬을 지원하지 않음
 - 수치형 타입이 많을때 시간이 더 오래걸릴 수 있다.
Over-sampling 
- SMOTE
precision = TP/(fp+TP)
recall = TP/(fn_tp)
F1 Score = 2((precisionrecall)/(precision+recall))

1. Gradient Boosting Model 
- 이전의 실수를 보완하는 sequential한 모델 
- 다음의 트리를 random하게 만들지 않음
- 컴퓨팅 리소스를 엄청 먹음

2. XGBoost - 파이썬래퍼, 사이킷런래퍼가 있음. 메서드 형식이 약간씩 다르므로 주의.
- GBM 대비, 병렬처리, 가지치기 등을 통해 수행시간이 빨라짐 
- 과적합 규제 (regularization) 기능 가짐
- 조기종료 (early stopping) 기능 가짐 : 트리를 sequential하게 만들 때, 로스가 더 이상 개선되지 않으면 스탑
- RF와 다르게 무작위성이 없음, 고차원 희소 데이터에도 비교적 잘 잘동함
- 최적화시킬 손실함수에는 Squared / Absolute / Huber / Quantile loss가 있음
        - Huber/Quantile loss : Squared/Absolute loss와 모양이 비슷하지만 이상치에 덜 민감함
        - Absolute loss도 많이 사용하지만 Squared loss를 더 많이 사용하는 이유 : 
                * squared : 모든 구간에서 differential, loss가 줄어들 때 기울기가 0에 가까워지는 것을 확인가능
                * absolute : loss에서는 기울기의 부호가 바뀌기 전까지 loss의 변화를 감지할 수 없음
- GOSS : Gradient-based One-side Sampling  (similar to reducing number of rows)
        데이터에서 큰 Gradient를 가진 인스턴스를 유지함 
        (기울기가 크다 = loss가 크다) & (그런 인스턴스를 유지함) = Leaf-wise 분할, One-side Sampling
        최대 손실값을 갖는 리프 노드를 지속적으로 분할하여, 트리를 깊게 만듦
        기울기가 큰 건 유지, 반대로 작은 건 또 작기 때문에 가중치를 적게 줌.
- EFB : Exclusive Feature Bundling : 배타적 특성 묶음 (similar to reducing number of columns)
        대규모 feature 수를 다루기 위함. 
        비슷한 value를 가진 feature끼리 set처럼 하나로 묶어서 생각해줌
- GOSS, EFB : 히스토그램 기반 - 구간 중 하나로 누락된 데이터를 대체함 - 결측치 따로 처리 안 해줘도 됨! 

(참고) 대부분 트리는 level-wise 분할 / LightGBM은 leaf-wise 분할 (균형을 맞추지 않고, 최대 손실을 유지하면서 분할)

3. 불균형한 데이터를 Resampling 해서 비율을 맞춰 줌
1) under-sampling
2) over-sampling
SMOTE : Synthetic Minority Over-Sampling Techinique
k-근접 이웃을 합쳐 새로 값을 생성해 줌 : k-근접 이웃 중간에 위치

4. 분류 - 불균형 데이터 평가 지표
1) 정확도 : 분류 문제의 대표 평가 지표
하지만 만약 데이터셋이 95:5 정도로 편향되었다면, 모두 A로 찍어도 95% 정확도를 갖는다는 함정이 있음 
이런 불균형 데이터가 많은 현실 세계에서는 정확도만으로 평가하기 어려움
2) 혼동행렬도 있음
