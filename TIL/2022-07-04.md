- ANN(Artificial Neural Network)
- 인간 개입의 여부
  - ML: 인간이 직접 특징을 도출할 수 있게 설계
  - DL: 스스로 일정 범주의 데이터를 바탕으로 공통된 특징을 도출
- 퍼셉트론(Perceptron)
  - XOR 문제 -> 복층 퍼셉트론으로 해결
- 순전파(forward propagation) / 역전파(back propagation)
  - 최적화된 가중치를 찾아가며 학습하는 방식 -> 오차가 최소가되는 방향
- 활성함수(Activation Func.)
  - 가중치 생성
  - 선형 결합을 비선형(또는 선형) 결합으로 변환하는 역할
- Gradient Vanishing (기울기 소실): 역전파 과정에서 입력층으로 갈수록 기울기가 점차 작아지는 현상
- Gradient Exploding (기울기 폭주): 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되며 발산하는 현상
- 딥러닝의 학습과정
  - 출력값과 실제값을 비교해, 그 차이를 최소화하는 가중치(W)와 편향(bias)의 조합 찾기
  - 가중치는 오차를 최소화하는 방향으로 모델이 스스로 탐색(역전파)
  - 오차 계산은 실제 데이터를 비교해, 손실함수를 최소화하는 값 탐색
  - 옵티마이저로 경사하강법 원리를 이용
- 손실함수(Loss Func.)
  - 실제값과 예측값의 차이를 수치화해주는 함수
  - Crossentropy는 값이 낮을수록 예측을 잘한것임
  - 분류
    - 이항분류: BinaryCrossentropy (이진 엔트로피)
    - 다항분류
      - CategoricalCrossentropy (교차 엔트로피)
      - CategoricalHinge (범주형 힌지 손실)
      - Hinge (힌지 손실)
      - KLDivergence (Kullback-Leibler 발산 손실)
  - 회귀
    - CosineSimilarity (코사인 유사도)
    - Huber (Huber 손실)
    - LogCosh (예측 오차의 쌍곡선 코사인의 로그)
    - MeanAbsoluteErr
    - MeanAbsulutePercentageErr
- 옵티마이저 (Optimizer)
  - 데이터와 손실함수를 기반으로 모델이 업데이트되는 방식
- 경사하강법 (Gradient Descent)
  - 가중치를 움직이며 최솟값을 찾는 방법 (최적화 방법)
- 확률적 경사하강법(SGD, Stochastic Gradient Descent)
  - 랜덤하게 추출한 일부 데이터에 대해 가중치를 조절
  - 속도는 개선되었지만, 극소 문제(loval minima)가 남아있음
- ANN(Artificial Neural Network)
  - 입력층(Input) -> 은닉층(Hidden) -> 출력층(Output)
- DNN(Deep Neural Network)
  - ANN을 기반으로 은닉층을 많이 늘림
- 출력층에서의 활성화 함수
  - 분류: Sigmoid, Softmax
  - 회귀: 항등함수(identity func.)
- 밀집층(Dense layer, FC)
  - 모든 뉴런에 연결되는 계층
  - 과적합 확률이 높아, 드롭아웃(Dropout) 기법을 이용
- loss: categorical_crossentropy = 원핫 벡터 라벨
- loss: sparse_categorical_crossentropy = 정수형 라벨

relu : 기울기 소실을 완화하기 위한 방법. 단점은 x가 0이하이면 항상 동일한 값(0) 출력
.Dropout() 오버피팅을 방지하기 위해 값을 버려주는 수치

CLASSIFICATION
SIGMOID : 이진분류
SOFTMAX : 다중 클래스 분류

sparse_categorical_crossentropy : 라벨이 정수형태

 categorical_crossentropy : one-hot vector

crossentropy loss = -(ylog(p) + (1 - y)log(1 - p)) 작을수록 좋은 지표

- 정확도의 함정: 정확도가 좋으면 뛰어난 모델이지만 치명적인 상황을 놓치는 경우가 있다.
- SMOTE: Oversampling의 기법 중 하나로 KNN을 이용하여 샘플의 수를 늘려준다.

딥러닝
Deep Learning: 입력층과 출력층 사이에서 은닉층으로 이뤄진 신경망!!(like 뉴런)
순전파와 역전파

시그모이드, 하이퍼볼릭탄젠트 함수


o 전반적 과정 : 레이어 구성 → 컴파일 → fit
 - Gradient로 Loss가 어느정도인지 보고,
 - Weight와 Bias 업데이트 반복

o 최적화 (Optimizer)
 - 

o 손실함수(Loss Funtion)
 - 분류 또는 회귀에 따라 손실함수 종류나 메트릭이 달라진다.
 - 엔트로피 등을 사용 (이항분류 BinaryCrossentropy 등)

ANN: 인공 신경망 기계학습 알고리즘.(단점: 최적 파라미터 찾기 어렵다. overfitting, 학습시간이 상대적으로 느림)
DNN: 모델내 은닉층을 많이 늘려서 학습 결과 향상시키는 알고리즘.
CNN: 데이터의 특징을 추출하여 특징의 패턴을 파악하는 알고리즘.(CL, PL 복합적으로 구성)
RNN: 내부에 순환구조가 있는 알고리즘. 현재의 학습과 과거의 학습이 영향을 주고받음.
활성화함수: 입력된 데이터의 가중 합을 출력신호로 변환하는 함수.

기울기 소실 문제: 역전파 알고리즘에서 처음 입력층으로 진행할수록 기울기가 점차적으로 작아지다 점점 변화가 없어지는 문제를 말함.(해결방법 ReLU, Leaky ReLU 사용)

프리시젼-양성가운데 맞춘 양성의 수!
리콜-실제로 양성인 것 중에 양성인 것은 몇개!
smote-양성음성을 분류하는 데이터는 불균형한 데이터가 많다.

우리가 아무리 피처엔지니어링을 잘 해도 데이터가 불균형하면 성능을 높일 수가 없다, 그때는 오버샘플링기법을 활용하자. 오버샘플링으로 레이블의 비율을 맞춤.
-------------------------------------
머신러닝과 딥러닝은 불러오는 방법은 비슷하지만 사용되는 알고리즘이 다르다.
cnn-이미지데이터
rnn-자연어처리 및 시계열 데이터에 유용하게 쓰임
------------------------------------------
<딥러닝기초>
퍼셉트론: 인간의 두뇌에서 영감을 받아서 탄생


시그모이드->y값이 0~1사이에 값이다, 좌우상하대칭, 변곡점, 기울기는 양수, 느리지만 세밀하게 학습
하이퍼볼릭탄젠트 ->시그모이드와 비슷하게 생겼지만 차이점은
->-1~1사이의 값을 가짐, 상대적으로 빠르게 학습

기울기 소실-기울기를 통해 오차를 구하고 계속 학습을 돌림, 그러다가 기울기 즉 신호가 사라짐 그것이 기울기 소실

crossentropyloss : -(y * log(p) + (1 - y) * log(1 - p)) / y=실제값(true), p=예측값(predict)
크로스엔트로피 값이 낮을수록 잘 예측한 값이다.
--------------------------------------------
mnist로 손글씨 분류 - 텐서플로우 내장 데이터로 우체국 우편번호 손글씨를 모아놓은 데이터

이미지를 1차원으로 flatten 해서 펴주면 어떤 문제가 있나?
데이터 크기가 커지고, 주변 정보(공간정보를 잃어버린다), 한 픽셀씩 처리해서 오래걸린다.
dense : 하이퍼파라미터로 몇 개를 고를건지.
dropout: 오버피팅을 방지하기 위해서 일부 노드를 지움.

np.argmax([]) 가장 큰 숫자의 인덱스를 반환해주는 함수.
earlystopping (조기종료)-ex)3회 연속 손실 개선이 없을 때 훈련 종료
model.compile-잘 모르겠으면 Adam 적용.


sparse_categorical_crossentropy 와 categorical_crossentropy 의 차이점?
categorical_crossentropy는 라벨이 원핫 벡터, 
sparse_categorical_crossentropy는 라벨이 정수형태다.

validation_split=0.2 를 fit에 추가했을 때 출력에 달라진 점?
val_loss, val_accuracy가 추가

CRUD-Create Read Update Delete(데이터 조회,수정, 삭제)

RGB - 빛의 삼원색, 다 합치면 흰색
CMYK - 색의 삼원색, 다 합치면 검정색
