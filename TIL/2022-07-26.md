역순 정렬= [::-1]
argsort: 정렬된 인덱스 값 반환
argmax: 가장 큰 레이블 값 반환
----------------------------------------------------
오거닉: 광고효과를 뿌리지 않고, 스스로 온 고객

AARRR:첫방문->첫회원가입->재구매->친구소개->첫구매

코호트분석: 분석 전에 데이터 셋의 데이터를 관련 그룹으로 묶어주는 것
코호트=집단

히트맵-전체적으로 보고싶을 때
판다스-컬럼별로 보고싶을 때

비음수 행렬 분해
음수를 포함하지 않은 행렬 V를 음수를 포함하지 않은 행렬 W와 H의 곱으로 분해하는 알고리즘
Topic 분류

LDA(LatentDirichletAllocation)
잠재 디리클레 분석
주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지 확인

LatentDirichletAllocation(n_components=topic 개수(len_topics), random_state=42)

Topic modeling
문서 집합의 추상적인 "주제"를 발견하기 위한 통계적 모델

PyLDAvis
시각화

cosine_similarity
특성 공간(feature space) 상에서 거리를 이용해 두 문서의 유사성(similarity)을 측정하는 방식

Online Retail
AARRR 
첫 방문 → 첫 회원가입 → 재구매 → 친구소개 → 첫구매

코호트 분석 
데이터 세트의 데이터를 관련 그룹으로 나누는 분석
✔️시간집단 특정 기간동안 제품이나 서비스에 가입한 고객
행동집단 과거에 제품을 구매했거나 가입한 고객
규모집단 다양한 규모의 고객

잔존율 분석
리텐션(재구매율, 재사용율)

RFM 분석
Recency, Frequency, Monetary analysis
Customer Segmentation

ARPU(Average Revenue Per User) :
가입한 서비스에 대해 가입자 1명이 특정 기간 동안 지출한 평균 금액

ARPPU(Average Revenue Per Paying User):
지불 유저 1명 당 한 달에 결제하는 평균 금액을 산정한 수치

- 특이값 분해를하면 정보를 잃어버릴 수 있는데, 용량이 작아지고 정보가 요약됨
- 토픽 모델링
- 잠재 의미 분석(LSI)
- pyLDAvis
- 코사인 유사도
- df.loc[조건, 열]
- RFM 분석
- ARPU / ARPPU

AARRR
- 현 시점에서 가장 핵심적인 지표에 집중할 수 있게 함
코호트 분석
- 고객의 수명 주기 전반에 걸쳐 패턴을 명확히게 볼 수 있음 (online retail -> 시간집단)
잔존율 분석 (retention)
- 고객이 이탈하는 방법과 이유를 이해하기 위해 사용자 메트릭을 분석

AARRR
- Acquisition : 어떻게 우리 서비스를 접하고 있는가
- Activation : 사용자가 처음 서비스를 이용할 때 긍정적인 경험을 제공하는가
- Retention : 이후의 서비스 재사용률은 어떻게 되는가
- Referral : 사용자가 자발적 바이럴, 공유를 일으키고 있는가
- Revenue : 최종 목적(매출)으로 연결되고 있는가

라이브DB에 바로 접근하지 않고 백업 DB로 분석하는 이유
- 현재 운영 중인 서비스에 영향을 최소화하기 위함
- 서버 과부화를 막기 위함
- 아무나 모든 유저의 정보를 열어볼 수 있는 것은 문제가 될 수 있음

ex) select * from user 이렇게 라이브DB에 날리면 모든 유저들의 데이터가 전부 불러와져서 서버에 과부화가 생기고  서비스에 장애가 생겨 회원가입도 안되고 서비스 자체가 운영이 안되는 일이 벌어질 수 있다.... > 이런 경우에는 해당 부분을 찾아서 kill시키면 됨

Stack / unstack 개념 정리
