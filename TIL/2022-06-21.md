o Bagging
 - bootstrap aggregating의 약자
 - 부트스트랩을 통해 조금씩 다른 훈련 데이터에 대해 
훈련된 기초 분류기(base learner)들을 결합(aggregating)시키는 방법
 - 서로 다른 데이터셋들에 대해 훈련시킴으로써, 트리들을 비상관화시키는 과정

o 앙상블 방법(Ensemble Machine Learning Approach)
 - 일련의 저성능 분류기를 결합한 복합 모델

o 연/월/일/시/분/초 만들기
 - pd.to_datetime()
 - df["year"] = df["datetime"].dt.year

o 평가지표
 - MSE, MAE, RMSE, RMSLE(로그, 특정값의 에러크기 조정) 등
 
  
bagging
bootstrap aggregating
  주어진 훈련 데이터에서 중복을 허용하여 원 데이터셋과 같은 크기의 데이터셋을 만듦
  트리들의 편향은 그대로 유지, 분산은 감소시켜 성능 향상
  트리들이 서로 상관화correlated 되어 있지 않다면 트리들의 평균은 노이즈에 강인

bagging 훈련과정
1. 부트스트랩으로 T개의 훈련 데이터셋 생성
2. T개의 트리들을 훈련
3. 트리들을 랜덤포레스트로 집계 (평균/과반수)

앙상블 방법
  여러 방법을 섞어 사용
 
트리의 깊이
  시작 노드 제외

Random Forest
  앙상블 방법

kaggle bike sharing demand
RMLSE =√ 1/𝑛∑(log(𝑝𝑖+1)−log(𝑎𝑖+1))^2
-∞가 되지 않게 +1
log1p


df를 train, tail로 나누기
1) df.loc[df["count"].notnull(), feature_names]
2) df.iloc[train.index][feature_names]

  평가
cross_validate(scoring="")
sklearn.metrics.SCORERS.keys()에서 찾은
평가 지표로 score을 확인할 수 있다.(y_predict는 y_valid_predict)

보통 잘섞어 쓰지만
train, test 데이터가 섞이면 의미가 없어 == 순서대로 나누어 주어야하는 데이터는 무엇이 있을까요?
시계열 데이터가 있을때

엔트로피 공식-> log
클래스가 8개일때 최대 엔트로피는 몇 일까요? == 3, this is log you know what? oh~ ok i see 

로그(log)는 지수 함수의 역함수이다. 
지수-제곱이 있는 위치, 밑이 2,10,e (e==무리수, 자연로그)
어떤 수를 나타내기 위해 고정된 밑을 몇 번 곱하여야 하는지를 나타낸다고 볼 수 있다.


밑이 2인 로그를 무엇으로 부를까요?
이산로그, 이진로그
밑이 10인 로그를 무엇으로 부를까요?
상용로그


측정 공식에 왜 log를 넣어주었을까요?
큰 수치를 예측할수록 오차가 커질수밖에 없고 그러면 오차 계산에서 비중이 커지는걸 방지하기 위해

1. RandomForest: 분류, 회귀분석 등에 사용되는 앙상블 학습 방법의 일종
                            다수의 결정트리-> 평균 예측치 출력
2. sklearn.model_selection: 교차 검증을 위한 학습/테스트용 분리, 그리드 서치로 최적 파라미터 추출 등의 API 제공하는 모듈(cross_val_score, GridSearchCV의 경우 내부 스코어링 전략에 의존)
3. sklearn.metrics: 분류, 회귀, 클러스터링, 페어와이즈(Pairwise)에 대한 다양한 성능 측정 
  가. 메트릭 함수에서 스코어링 전략
     1) _score로 끝나는 함수: 값을 최대화 할수록 좋음
     2) _error or _loss로 끝나는 함수: 값을 최소화 할수록 좋음
     참고: https://runebook.dev/ko/docs/scikit_learn/modules/model_evaluation
sklearn.ensemble: 앙상블 알고리즘 제공(Random Forest, boost계열 등)

배깅(bagging) - bootstrap aggregating의 약자 
부트스트랩을 통해 조금씩 다른 훈련 데이터에 대해 훈련된 기초분류기들을 결합시키는 방법
트리들을 비상관화 시켜주는 과정
트리들이 상관화 되어 있지 않다면 평균은 노이즈에 대해 강인
랜덤포레스트 - sklearn.ensemble
밑이 2인 로그 : 이진로그
밑이 10인 로그 : 상용로그
RMSLE : Root Mean Squared Logarithmic Error

1. 랜덤포레스트 : 서로 다른 의사결정 트리를 앙상블 기법을 통해 여러 번 반복 훈련 시킴
        - Bagging (Bootstrap Aggregation) 방식 이용 
                -> 여러 개 훈련 데이터 셋 생성
                -> 각 트리마다 훈련, 예측 
                -> 앙상블 기법 이용하여 예측 결과 집계, 하나의 분류기로 만듦
                (cf. Bagging : parallel / Boosting : sequential한 차이가 있음)
        - n_estimators (트리의 갯수) / max_depth : 예측 시간, 성능 등에 가장 큰 영향을 끼침
        - 참고로 얘는 아직 싸이킷런에서 제공하는 시각화 도구 (plot_tree같은) 없음!

        (cf. Bootstrap : 주어진 훈련 데이터에서 중복을 허용하여, 같은 크기의 데이터 셋을 만듦
                - 트리 편향은 유지하면서 분산 감소시킴 : 성능 향상 (노이즈 등에 robust해짐))

2. 교차 검증, 피쳐 엔지니어링, 하이퍼 파라미터 튜닝(수동, GridSearch, RandomizedSearch 등)에 대해 복습 및 예제 실습 진행함
        - 랜덤 포레스트 : k개 fold로 쪼개어, n번 반복시키면서 총 k*n 번 학습시킬 수 있음
        - 오히려 피쳐를 빼주었을 때 성능이 올라가기도 하는 걸 봐서는, 엔지니어링이 중요함
        - 
RandomizedSearchCV
verbose: 로그를 찍는 파라미터이다.
n_inter: 빨리 결과값을 보고 싶다면 기본값인 10에서 그 이하로 줄일 수 있다.
n_job: 사용할 코어의 수 (-1로 지정하는 이유는 다른 사용자의 코어의 수가 다 다르기 때문이다.)
n_iter=5, cv=5 총 25번을 학습한다.
*clf.bestestimator
*clf.bestscore
*pd.DataFrame(clf.cvresults).sort_values(by="rank_test_score", ascending=True)

1. 배깅
bootstrap aggregating의 약자
부트스트랩을 통해 조금씩 다른 훈련 데이터에 대해 훈련된 기초 분류기들을 결합
서로 다른 데이터셋들에 대해 훈련시킴으로써 트리들을 비상관화 시켜주는 과정

2. 부트스트랩
훈련 데이터에서 중복을 허용하여 원 데이터셋과 같은 크기의 데이터셋을 만드는 과정
- 트리들의 편향은 유지하면서 분산은 감소시키기 때문에 포레스트의 성능을 향상
- 트리들이 서로 상관화(correlated)되어 있지 않다면 여러 트리들의 평균은 노이즈에 대해 강인
- 여러개의 트리를 만들 수 있기 때문에 오버피팅이 덜 일어날 수 있음

3. Bagging 훈련과정
- 부트스트랩 방법을 통해 T개의 훈련 데이터셋을 생성 - 부트스트랩
- T개의 기초 분류기(트리)들을 훈련 
- 기초 분류기(트리)들을 하나의 분류기(랜던 포레스트)로 집계(평균 또는 과반수투표 방식 이용) - 집계(aggregating)

4. 앙상블 방법(Ensemble Machine Learning Approach)
- 앙상블은 개선된 분류기를 생성하기 위해 일련의 저성능 분류기를 결합한 복합 모델
- 과반수 투표를 수행하는 개별 분류기 투표 및 최종 예측 레이블이 반환
- 배깅 접근법을 사용하여 분산을 줄이고, 부스팅 접근법을 사용하여 편향을 사용 또는 스태킹 접근법으로 예측 계선

5. 랜덤 포레스트
- 분류 회귀 에 사용되는 앙상블 학습 방법의 일종 (cart)
- 주요파라미터: n_estimators : 트리의 수/ criterion: 가지의 분할의 품질을 측정

6. 회귀 평가방법
 R2스퀘어MAE/MASE/RMSE/RMSLE
- R² 는 값이 클수록 성능이 좋음
- MAE/MASE/RMSE/RMSLE는 값이 작을수록 회귀 성능이 좋은 것. 예측값과 실제값의 차이가 없다
- 분류와 회귀는 scoring 하는 방법이 다르다

7.순서대로 나누어 주어야하는 데이터 : 시계열이 있는경우

8. pd.DataFrame(clf.cvresults)
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_train, y_valid_predict)

- 부트스트랩(bootstrap):
중복추출하여 원본 데이터 셋과 비슷한 셋을 추출하는 것
- 배깅(bagging) : 
bootstrap aggregating의 약자
서로 다른 데이터셋들에 대해 훈련 시킴으로써, 트리들을 비상관화시켜 주는 과정
- 앙상블(ensemble) : 다수의 트리를 만들어서 학습하는 방법
- RMSLE(Root Mean Squared Logarithmic Error)
- Log의 해석
