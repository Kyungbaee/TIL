- 자연어 처리 (NLP)
- (주로) 텍스트를 벡터화하는 방법
  - 머신러닝: 단어의 빈도수
  - 딥러닝: 시퀀스 방식의 인코딩
- 정규 표현식
- 토큰화(Tokenization):
  - 텍스트 조각을 토큰이라 함
  - 패턴을 찾는데 유용
- 정제(Cleaning): 노이즈 제거
- 정규화(Normalization): 표협 방법이 다른 단어들을 같은 단어로 만듦
- 어간 추출(Stemming): 단어 형식을 의미가 있거나 무의미할 수 있는 줄기로 축소 (원형을 유지하지 않음)
- 표제어 표기법(Lemmatization): 언어학적으로 유효한 의미로 축소 (원형을 유지함)
- n-gram은 CNN에서 주변 정보를 이용하는것과 유사함
- BOW(Back Of Word)에서 순서를 고려하지 않는 단점을 보완 -> N-Gram
- 단어 빈도만 고려했을 경우, 불용어의 가능성이 높아짐

ngram_range : 토큰을 몇 개 사용할지 구분. 
ngram_range(1, 2) 이면 1이상 2이하 토큰
min_df : 특이 단어 제거, 우연히 등장하는 의미 없는 단어 제외
max_df : 빈번하게 등장하는 불용어 제거 용이
max_features : 벡터라이저가 학습할 기능(어휘)의 양 제한, corpus중 빈도수가 가장 높은 순으로 해당 갯수만큼만 추출
stop_words : 불용어

자연어 처리
자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일
기계에게 인간의 언어를 이해시키는 인공지능의 한 분야
분류: 스팸메일, 뉴스 기사
군집화 : 비슷한 문의 모으기
회귀 : 뉴스 기사로 주가 예측
차원축소 : 시각화

전처리 → 토큰화 → 토큰-ID

TF term frequency 단어 빈도
DF document frequency 문서 빈도
IDF 역문서 빈도(DF의 역수)
TF-IDF TF*IDF

min_df
오타 제거, 한두번 나오는 의미없는 단어 제거
max_df
불용어 제거

Bag of Words(BOW): 단어 단위로 얼마나 많이 나타나는지 세는 방법
BOW의 단점?!! -> 단어의 순서가 완전히 무시된다...
Ex) its bad, not good at all./ its good, not bad at all.

이 단점을 극복하기 위해서 N-grams 개념 등장
N-grams: 단어를 묶어서 n개의 숫자만큼 토큰을 묶어서 사용!
ngram_range(1,3): 단어 수가 1개에서 3개까지 묶어서 사용
N-grams 단점?! -> 의미가 너무 없고 불용어가 너무 많이 나온다.

이 단점을 극복하기 위한 TF-IDF 개념 등장!
TF(Term Frequency): 단어의 빈도 수!
DF(Document Frequency): 얼만큼 많은 문서에서 등장하는가?

TF가 높으면 가중치가 높고, DF가 높으면 너무 흔해서 가중치가 낮다
IDF는 DF의 역수!
그래서 TF-IDF!!가 높으면 가중치가 높다.

min_df : 빈도수 적은 단어 제거 역할
max_df : 불용어 제거 역할
stop_words: 불용어 지정

1. 자연어처리(NLP)
- 자연어(natural language): 우리가 일상에서 사용하는 언어
- 자연어 처리: 자연어 의미를 분석하여 컴퓨터가 처리할 수 있도록 함
        - 음성 인식, 내용 요약, 번역, 감정 분석, 텍스트 분류(스팸 메일 등) 등
        1) 머신러닝에선 '단어의 빈도수' 위주 
                - 앞뒤 맥락이 사라진다는 단점 (like flatten in CNN)
                - 자연어 분류 과정
                        (1) 데이터 로드
                        (2) 텍스트 데이터 전처리
                        (3) 데이터 나누기(train, test) > 텍스트 데이터 벡터화
                                - 상황에 따라 바뀔 수 있음, 순서가
                                - 근데 실제 비즈니스에서는 테스트에 어떤 데이터가 있을 지 모름
                                  따라서 보통 나눈 뒤에 벡터화를 진행함
                                - 벡터화: 원핫인코딩에 따라 BOW, TF-IDF 방식이 존재
                - 기존 ML 과의 차이는 텍스트데이터 벡터화(기계가 이해할 수 있는 수치 형태로 변환)
        2) 딥러닝에서는 '시퀀스' 방식 인코딩
- 자연어 처리: 전처리가 굉장히 어려움
        - 문법 및 띄어쓰기가 맞지 않음
        - 어떤 내용이 들어올 지 모름 등등

2. 텍스트 전처리(정규화라고도 함_ 표현 방법이 다른 단어를 같은 단어로 만듦)
- 기계가 텍스트를 이해할 수 있도록, 전처리를 통해 신호와 소음을 구분 
- 이상치 기인 오버피팅을 방지
        1) 불용어(stopword)
                - 한국어는 프로젝트에 따라 불용어가 달라지기 때문에 그 때마다 만들어 사용
                (영어는 불용어 사전이 따로 존재함)
                - 언어 분류, 스팸 필터링, 캡션 생성, 자동 태그 생성, 감정 분석, 텍스트 분류 등 : 제거
                - 기계 번역, 질문 답변 문제, 텍스트 요약, 언어 모델링 등 : 제거 비추천 
                        (응용 프로그램의 중요한 신호가 될 수 있음)
        2) 어간 추출(stemming)
                - 빈도수 기반 분석 시, 나너/은는이가 등 큰 의미가 없음에도 빈도가 높은 애들이 소음이 될 수 있음
                - 또는 했음/했습니다/합니다 등 같은 의미임에도 다른 벡터로 분류되어 메모리 잡아먹을 수 있음
        3) 표제어 표기법(lemmatization)
                i.g., create / creating / created / creats 
                        -> stemming : creat, lemmatization : create
                (표제어 표기법은, 어간 추출과 달리, 원형을 유지한다는 장점)
        4) 정규 표현식(regular expression)
                - 하나의 프로그래밍 언어임! 주의! (특정한 규칙을 가진 문자열의 집합을 표현)
                - 주요 메타문자
                        [^ ] 부정 : 문자 클래스 안의 문자를 제외한 나머지를 선택 
                        $ 끝, ^ 처음 (문자열이나 행의)
                        () 하위식 _ abc|adc == a(b|d)c
                        * 0회 이상 / + 1회 이상 / {m,n} m회 이상, n회 이하 '특정 텍스트' 등장
        5) 토큰화(tokenization)
        텍스트 조각을 토큰이라는 '더 작은 단위'로 분리함
        (토큰: 문자열 분석을 위한 단위_ 단어, 문자 또는 하위 단어)
        - 토큰생성(tokenizing)
                - 자연어 처리 도구 nltk (한국어 지원 안 함! 영어 분석할 때 굿)
                - 한국어 형태소 분석기 KoNLPy
                        (자바, C 등 다른 언어로 만들어진 걸 python으로 래핑해놓은 거임.
                        따라서 컴파일 환경이 생성되어 있어야 사용 가능)
                  한국어 자연어 처리 파이썬 라이브러리 soynlp
                        (얘는 파이썬 온리)

3. 원핫인코딩
        1) BOW(Bag of Words) : 하나의 토큰을 이용 > 텍스트 데이터 수치화
                - 단어 출현 빈도(frequency) only 이용
                - 단어 순서는 전혀 고려하지 않음
> n-gram을 사용
                BOW를 사이킷런에서 구현한 라이브러리가 CountVetorization임
        2) N-gram 언어 모델(language model): n개의 token을 사용
                - uni-gram / bi-gram / tri-gram / bi-gram(1,2) / tri-gram(2,3) 등
                - 맥락에 대한 이해는 높아짐
                - 경우의 수가 너무 많아짐(희소 행렬이 너무 많아질 수 있음) > 연산량 부하 과다
> 따라서 차원 축소를 통해 압축시켜 줌
                        - 특이값 분해 등 (선형대수 통해 연산, 차원축소)

4. 문서 단어 행렬(DTM, Document-Term Matrix)
다수의 문자에서 등장하는 각 단어의 빈도를 행렬로 표현함
(각 문서에 대한 BOW를 행렬화함)
        - ngram_range _ default = (1,1)
        - min_df (int : 빈도 / float: 비율 의미)
                문서에 특정 단어가 '최소 몇 번' 이상 등장하는 것만 BOW에 담겠음
                - 우연히 생긴 오타, 특이 단어 등 제거에 용이함
        - max_df 
                - 불용어 제거에 용이

5. 단어 빈도(TF, Term Frequency)
특정한 단어가 문서 내에서 얼마나 자주 등장하는지 나타냄
        - 이 값이 높을수록 문서에서 중요하다고 판단 가능
(cf. 문서 빈도 DF, Document Frequency: 특정 단어가 등장한 문서의 수)
(cf. 역문서 빈도 IDF, Inverse Document Frequency)
(cf. TF-IDF : TF 랑 IDF 곱) 
        - TF-IDF 가중치 : 해당 단어가 특정 문장에선 등장할 경우 높게 측정됨

5. fit, transform
- 학습세트 : fit, transform (fit_transform) both 
- 예측세트 : transform only
        - 자연어 처리에서는 다르게 학습할 경우 순서가 바뀌면서 문제가 생길 수 있음
