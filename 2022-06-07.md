parquet : 열 기준으로 정리, 데이터 형식 통일되어 압축하여 보기에 편함
                 그냥 csv 형태에 비해 메모리 덜 잡아먹음 but 별도 프로그램 필요
Apache Spark = 빅데이터 분산처리 플랫폼
pyspark = 파이썬환경에서 쓸 수 있게해주는 인터페이스

plotly = 한글 사용 문제 없음
df에 문제 없는데 그래프 안그려지면 orient 확인(x, y축을 바꿔보자)
그래프 그릴 때, groupby로 df 먼저 전처리 해준 뒤 그리면 훨씬 빠르게 처리 가능

downcast = "integer" "signed" "unsigned" "float"
parquet : 일반적으로 csv파일 보다 크기가 작지만 용량이 작은 경우 메타데이터를 지니고 있어 csv파일 보다 클 수도 있다.
concat, merge, cufflinks

cufflinks: pandas와 plotly를 이어주는 도구
downcast: 도메인 값의 최대치를 고려하여 저장공간의 최적화를 위해 더 적은 공간을 사용 하는 방향으로 형 변환 시켜주는 파라미터 pd.to_numeric(Series, erroes='', downcast='')
parquet: 열중심 저장을 추구함으로 인해서 데이터 압축, 인코딩에 효율이 좋음, 크기가 작은 데이터프레임을 parquet형태로 저장하게 되면, 메타데이터로 인해 오히려 오버헤드 발생

  - 파일과 메모리 용량을 효율적으로 관리하는 방법 (fastparquet, pyarrow, pandas downcast, koalas)
  - parquet: 행기반이 아니라, 열기반 -> 같은 데이터 형식끼리 저장되기 때문에 압축률이 높아짐
  - koalas: pyspark에서 사용하는 대용량 관리 툴, pandas보다 더 큰 데이터를 분석할 수 있는 툴

downcast로 용량 줄이기
int : unsigned
float : float
bool : int8
object : category
pd.to_numeric(df[col], downcast="")

parquet
열 지향 데이터 파일 형식
df.to_parquet(df, compression="gzip)

downcast : 메모리 용량을 줄이기 위해 사용되는 것
    - bool → int8, int → unsigned, float → float로 downcast
    - 사용법 : pd.to_numeric(df, downcast=’ ’)
.to_numeric(df, errors= ’ignore / coerce / raise’)
    <error 종류>
    - ignore : 숫자로 변환할 수 없는 값이면 원본 데이터를 그대로 반환
    - coerce : 숫자로 변환할 수 없는값이면 기존 데이터를 지우고 NaN으로 반환
    - raise : 숫자로 변경할 수 없는 값이면 오류 일으키며 코드 중단
.to_parquet : parquet은 컬럼기반 저장포맷으로 필요한 데이터를 읽어 데이터 크기를 줄임
.isnull().mean() * 100 : 결측치 비율
.value_counts() : 빈도수 구하는 법
histfunc="count” : plotly를 통해 히스토그램에서 빈도수를 보고자 할 때
    - px.histogram(df, x=’ ’, y=’ ’, histfunc=’count’)
    - sns.barplot(data=df, x=’ ’, y=’ ’, estimator=np.sum) → seaborn으로 시각화
histfunc="sum” : plotly를 통해 히스토그램에서 합계를 보고자 할 때
    - px.histogram(df, x=’ ’, y=’ ’, histfunc=’sum)→px(plotly.express)
데이터가 클 때 시각화를 빠르게 하는 방법 : groupby or pivot table로 변환한 후 시각화

<메모리 줄이는 방법>
1) 반복문, downcast를 사용해서 줄이기
fcols = df.select_dtypes('float').columns 
icols = df.select_dtypes('integer').columns 
bcols = df.select_dtypes('bool').columns 
df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float') 
df[icols] = df[icols].apply(pd.to_numeric, downcast='unsigned') 
df[bcols] = df[bcols].apply(pd.to_numeric, downcast='int8')

2) 불러올 때부터 타입을 바꿔서 불러오기
df2 = pd.read_csv("data/nhis_drug_sample_2020_3.csv", dtype={"기준년도":"int16", "가입자 일련번호": np.int16})
- 데이터의 col이 어떤 게 있는지 모두 알고 있어야 함

* signed/unsigned 차이
ex) int8 일때,
signed는 부호가 있어서 표현 범위가 -128 부터 127 까지라면,
unsigned는 부호가 없어서 0부터 255까지 표현할 수 있는 것
(둘의 표현 크기는 같지만 범위가 다른 것!)

<효율적인 데이터 압축>
Q. 왜 행 기반이 아니고 열 기반 저장 형식을 사용하면 용량이 더 줄어들까?
 - 열 기반으로 저장하면 같은 형식끼리 저장이 되기 때문에 압축률이 훨씬 높다
   (int, float, object…)

<error의 3가지 옵션>
1) errors = 'ignore' 
-> 만약 숫자로 변경할 수 없는 데이터라면 숫자로 변경하지 않고 원본 데이터를 그대로 반환
2) errors = 'coerce' 
-> 만약 숫자로 변경할 수 없는 데이터라면 기존 데이터를 지우고 NaN으로 설정하여 반환
3) errors = 'raise' 
-> 만약 숫자로 변경할 수 없는 데이터라면 에러를 일으키며 코드를 중단

downcast - 데이터크기가 커서 처리가 힘들 때 데이터 양을 줄여주는 과정 중 하나. 예를들어 int64를 사용하고 있는 데이터가 사실 int8로 모두 표현이 가능하다면 int8로 모두 바꿔주면 획기적으로 데이터를 줄일 수 있을 것이다. 그럴 때 사용 가능.
cufflinks-  plotly 와 pandas 를 이어서 pandas에서 plotly 의 그래프를 쉽게 그릴 수 있게 해주는 도구
parquet 
- 효율적인 데이터 저장 및 검색을 위해 설계된 오픈 소스, 열 지향 데이터 파일 형식
- 열 스토리지, 필요한 데이터만 읽기
- 가끔 csv파일보다 용량이 큰 경우가 있는데 이 경우는 원본 데이터 양이 원래 적어서 parquet안에 들어가있는 메타데이터 때문에 csv파일보다 용량이 크게 보이게 된다.
- 기본 압축값은 지정하지 않으면 snappy. gzip의 퍼포먼스가 이보다 좋아서 실습 때 gzip으로 사용.
merge
- key 값이 동일하게 있으면 on 사용하지 않아도 자동으로 합쳐준다. 왼쪽 오른쪽 등만 설정.
to_numeric 옵션 errors="coerce"
- 에러나는 부분을 강제로 nan값으로 채워준다.
