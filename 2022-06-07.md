parquet : 열 기준으로 정리, 데이터 형식 통일되어 압축하여 보기에 편함
                 그냥 csv 형태에 비해 메모리 덜 잡아먹음 but 별도 프로그램 필요
Apache Spark = 빅데이터 분산처리 플랫폼
pyspark = 파이썬환경에서 쓸 수 있게해주는 인터페이스

plotly = 한글 사용 문제 없음
df에 문제 없는데 그래프 안그려지면 orient 확인(x, y축을 바꿔보자)
그래프 그릴 때, groupby로 df 먼저 전처리 해준 뒤 그리면 훨씬 빠르게 처리 가능

downcast = "integer" "signed" "unsigned" "float"
parquet : 일반적으로 csv파일 보다 크기가 작지만 용량이 작은 경우 메타데이터를 지니고 있어 csv파일 보다 클 수도 있다.
concat, merge, cufflinks

cufflinks: pandas와 plotly를 이어주는 도구
downcast: 도메인 값의 최대치를 고려하여 저장공간의 최적화를 위해 더 적은 공간을 사용 하는 방향으로 형 변환 시켜주는 파라미터 pd.to_numeric(Series, erroes='', downcast='')
parquet: 열중심 저장을 추구함으로 인해서 데이터 압축, 인코딩에 효율이 좋음, 크기가 작은 데이터프레임을 parquet형태로 저장하게 되면, 메타데이터로 인해 오히려 오버헤드 발생

  - 파일과 메모리 용량을 효율적으로 관리하는 방법 (fastparquet, pyarrow, pandas downcast, koalas)
  - parquet: 행기반이 아니라, 열기반 -> 같은 데이터 형식끼리 저장되기 때문에 압축률이 높아짐
  - koalas: pyspark에서 사용하는 대용량 관리 툴, pandas보다 더 큰 데이터를 분석할 수 있는 툴

downcast로 용량 줄이기
int : unsigned
float : float
bool : int8
object : category
pd.to_numeric(df[col], downcast="")

parquet
열 지향 데이터 파일 형식
df.to_parquet(df, compression="gzip)

downcast : 메모리 용량을 줄이기 위해 사용되는 것
    - bool → int8, int → unsigned, float → float로 downcast
    - 사용법 : pd.to_numeric(df, downcast=’ ’)
.to_numeric(df, errors= ’ignore / coerce / raise’)
    <error 종류>
    - ignore : 숫자로 변환할 수 없는 값이면 원본 데이터를 그대로 반환
    - coerce : 숫자로 변환할 수 없는값이면 기존 데이터를 지우고 NaN으로 반환
    - raise : 숫자로 변경할 수 없는 값이면 오류 일으키며 코드 중단
.to_parquet : parquet은 컬럼기반 저장포맷으로 필요한 데이터를 읽어 데이터 크기를 줄임
.isnull().mean() * 100 : 결측치 비율
.value_counts() : 빈도수 구하는 법
histfunc="count” : plotly를 통해 히스토그램에서 빈도수를 보고자 할 때
    - px.histogram(df, x=’ ’, y=’ ’, histfunc=’count’)
    - sns.barplot(data=df, x=’ ’, y=’ ’, estimator=np.sum) → seaborn으로 시각화
histfunc="sum” : plotly를 통해 히스토그램에서 합계를 보고자 할 때
    - px.histogram(df, x=’ ’, y=’ ’, histfunc=’sum)→px(plotly.express)
데이터가 클 때 시각화를 빠르게 하는 방법 : groupby or pivot table로 변환한 후 시각화

<메모리 줄이는 방법>
1) 반복문, downcast를 사용해서 줄이기
fcols = df.select_dtypes('float').columns 
icols = df.select_dtypes('integer').columns 
bcols = df.select_dtypes('bool').columns 
df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float') 
df[icols] = df[icols].apply(pd.to_numeric, downcast='unsigned') 
df[bcols] = df[bcols].apply(pd.to_numeric, downcast='int8')

2) 불러올 때부터 타입을 바꿔서 불러오기
df2 = pd.read_csv("data/nhis_drug_sample_2020_3.csv", dtype={"기준년도":"int16", "가입자 일련번호": np.int16})
- 데이터의 col이 어떤 게 있는지 모두 알고 있어야 함

* signed/unsigned 차이
ex) int8 일때,
signed는 부호가 있어서 표현 범위가 -128 부터 127 까지라면,
unsigned는 부호가 없어서 0부터 255까지 표현할 수 있는 것
(둘의 표현 크기는 같지만 범위가 다른 것!)

<효율적인 데이터 압축>
Q. 왜 행 기반이 아니고 열 기반 저장 형식을 사용하면 용량이 더 줄어들까?
 - 열 기반으로 저장하면 같은 형식끼리 저장이 되기 때문에 압축률이 훨씬 높다
   (int, float, object…)

<error의 3가지 옵션>
1) errors = 'ignore' 
-> 만약 숫자로 변경할 수 없는 데이터라면 숫자로 변경하지 않고 원본 데이터를 그대로 반환
2) errors = 'coerce' 
-> 만약 숫자로 변경할 수 없는 데이터라면 기존 데이터를 지우고 NaN으로 설정하여 반환
3) errors = 'raise' 
-> 만약 숫자로 변경할 수 없는 데이터라면 에러를 일으키며 코드를 중단

downcast - 데이터크기가 커서 처리가 힘들 때 데이터 양을 줄여주는 과정 중 하나. 예를들어 int64를 사용하고 있는 데이터가 사실 int8로 모두 표현이 가능하다면 int8로 모두 바꿔주면 획기적으로 데이터를 줄일 수 있을 것이다. 그럴 때 사용 가능.
cufflinks-  plotly 와 pandas 를 이어서 pandas에서 plotly 의 그래프를 쉽게 그릴 수 있게 해주는 도구
parquet 
- 효율적인 데이터 저장 및 검색을 위해 설계된 오픈 소스, 열 지향 데이터 파일 형식
- 열 스토리지, 필요한 데이터만 읽기
- 가끔 csv파일보다 용량이 큰 경우가 있는데 이 경우는 원본 데이터 양이 원래 적어서 parquet안에 들어가있는 메타데이터 때문에 csv파일보다 용량이 크게 보이게 된다.
- 기본 압축값은 지정하지 않으면 snappy. gzip의 퍼포먼스가 이보다 좋아서 실습 때 gzip으로 사용.
merge
- key 값이 동일하게 있으면 on 사용하지 않아도 자동으로 합쳐준다. 왼쪽 오른쪽 등만 설정.
to_numeric 옵션 errors="coerce"
- 에러나는 부분을 강제로 nan값으로 채워준다.

# Downcast
1. Downcast.가 필요한 이유 : 불필요한 메모리 부담을 줄이기 위해서
    1. 메모리 크기에 따라 불러올 수 있는 데이터 사이즈에 한계가 있음
    2. 결국엔 데이터를 저장하는 것도 다 비용이랑 연결되기 때문
    3. 방법
        1. dtypes 지정 (dtypes.name, select_dtypes 등 이용): unsigned/integer/float/uint/int8 등
        2. 처음부터 dtype 지정해줌 : “int16” or np.int16 처럼 
            → 근데 얜 컬럼마다 일일이 지정해줘야 함

# Parquet
1. 메타정보 포함 (info를 통해 불러오는 column, dtype/크기 등의 정보)
    → 작은 사이즈 데이터는 오히려 parquet가 사이즈가 더 클 수 있음
2. 주의! 메모리 사용량 자체는 동일함!
    → CPU times, wall times 등 해당 요청을 처리하는 게 더 빠르단 거임
3. 데이터 압축 방식에 따라서도 사이즈 달라질 수 있음
    - 엔진 지정 : pyarrow, fastparquet 등 - 읽어오는 속도에 차이
        해당 라이브러리가 깔려 있어야 파케이 저장 가능
    - 압축 방법 : uncompressed, SNAPPY, GZIP 등
4. 단 압축을 많이 할 경우, csv 등 파일과 달리 주피터 노트북이나 엑셀 등에서 파일을 바로 열어 볼 수가 없게 됨. 파일을 열어보기 위해 전처리 등 별도의 작업이 필요

# Spark

1. 대용량 데이터를 처리하기 위한 분석 도구
    1. 개인 컴퓨터에 설치할 수도 있지만
    2. 회사에서는 공용 서버에 설치해서 사용하기도 함
    3. 예전에는 스파크 문법을 별도로 써야 했는데, Koalas 내장하게 되면서 스파크에서도 판다스와 같은 문법을 써서 분석 가능!
